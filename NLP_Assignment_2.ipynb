{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKROIJbPFoQK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "635db520-4b60-4962-c8a2-1411ad9bff7f"
      },
      "source": [
        "# Import the pandas library to read our dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "import random\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXCdoNTyDnLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_link = 'https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv'\n",
        "data_set = pd.read_csv(file_link, sep = '\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnlTImazEN_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "251f71c6-fca0-40ff-ad6e-1f48e4b62e2a"
      },
      "source": [
        "data_set.head(10)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>of escapades demonstrating the adage that what...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>of</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>escapades demonstrating the adage that what is...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>escapades</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>demonstrating the adage that what is good for ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "5         6  ...          2\n",
              "6         7  ...          2\n",
              "7         8  ...          2\n",
              "8         9  ...          2\n",
              "9        10  ...          2\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6809NAeQUQqM",
        "colab": {}
      },
      "source": [
        "def load_documents(documents, combined_phrases = False):\n",
        "  # extract full sentences only from the dataset\n",
        "  if combined_phrases:\n",
        "    fullSentences = []\n",
        "    curSentence = 0\n",
        "    for i in range(data_set.shape[0]):\n",
        "      if data_set['SentenceId'][i]> curSentence:\n",
        "        fullSentences.append((data_set['Phrase'][i], data_set['Sentiment'][i]))\n",
        "        curSentence = curSentence +1\n",
        "    return fullSentences\n",
        "  else:\n",
        "    raw_documents = []\n",
        "    for index, row in data_set.iterrows():\n",
        "      raw_documents.append([row['Phrase'], row['Sentiment']])\n",
        "    return raw_documents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90IpJWSgUtbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents = load_documents(data_set)\n",
        "documents = np.array(documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WY91SFDHUQCZ",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer\n",
        "import re\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stopwords_en = stopwords.words('english')\n",
        "punctuations = \"?:!.,;\\\"-()_'\" \n",
        "\n",
        "def preprocess_text(sentences, remove_stopwords = True, useStemming = False, useLemma = False, removePuncs = True):\n",
        "  new_sentences = list()\n",
        "  for sentence in sentences:\n",
        "    sentence = sentence.lower()\n",
        "    # Remove punctuations\n",
        "    if removePuncs:\n",
        "      sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "    sentence = sentence.split(' ')\n",
        "    tempSentence = []\n",
        "    for w in sentence:\n",
        "      newWord = w\n",
        "      if remove_stopwords and (w in stopwords_en):\n",
        "        continue\n",
        "      # if removePuncs and (w in punctuations):\n",
        "      #   continue\n",
        "      if useStemming:\n",
        "        # newWord = lancaster.stem(newWord)\n",
        "        newWord = porter.stem(newWord)\n",
        "      if useLemma:\n",
        "        newWord = wordnet_lemmatizer.lemmatize(newWord)\n",
        "      tempSentence.append(newWord)\n",
        "    new_sentences.append(' '.join(tempSentence))\n",
        "  return new_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjARqWRIOt9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_arrays(x_train, x_test):\n",
        "  x_train = x_train.toarray()\n",
        "  x_test = x_test.toarray()\n",
        "  return x_train, x_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tfDC1_nOQun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "def get_bag_of_words(x_train, x_test, ngram_range = (1,2), max_features = 1000):\n",
        "  vectorizer = CountVectorizer(stop_words = \"english\", ngram_range = ngram_range, max_features = max_features)\n",
        "  x_train_vector = vectorizer.fit_transform(x_train)\n",
        "  x_test_vector = vectorizer.transform(x_test)\n",
        "\n",
        "  return convert_to_arrays(x_train_vector, x_test_vector)\n",
        "\n",
        "def get_tfidf(x_train, x_test, ngram_range = (1,2), max_features = 1000):\n",
        "  vectorizer = TfidfVectorizer(stop_words = \"english\", ngram_range = ngram_range, max_features = max_features)\n",
        "  x_train_vector = vectorizer.fit_transform(x_train)\n",
        "  x_test_vector = vectorizer.transform(x_test)\n",
        "  \n",
        "  return convert_to_arrays(x_train_vector, x_test_vector)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvLgq-4tUfko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "def get_data(bag_of_words = True, tfidf = False, max_features = 1000):\n",
        "  if bag_of_words and tfidf:\n",
        "    raise(ValueError('Select only one method'))\n",
        "  elif not bag_of_words and not tfidf:\n",
        "    raise(ValueError('No method is selected'))\n",
        "  x_train, x_test, y_train, y_test = train_test_split(documents[:, 0], documents[:,1], random_state = 2003, test_size = 0.3)\n",
        "\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  x_train = preprocess_text(x_train)\n",
        "  x_test = preprocess_text(x_test)\n",
        "\n",
        "  if bag_of_words:\n",
        "    x_train, x_test = get_bag_of_words(x_train, x_test, max_features = max_features)\n",
        "  else:\n",
        "    x_train, x_test = get_tfidf(x_train, x_test, max_features = max_features)\n",
        "\n",
        "  return x_train, x_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4CXnmceT4pl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "  p = precision(y_true, y_pred)\n",
        "  r = recall(y_true, y_pred)\n",
        "  return (2 * p * r)/ (p + r + K.epsilon())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7UK8AQyrElK",
        "colab_type": "text"
      },
      "source": [
        "### Single Channel Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgf18sUmGrDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model 1 with individual phrases\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D,Dropout, MaxPooling1D, Flatten, Dense, Embedding, SpatialDropout1D\n",
        "\n",
        "class CnnModel():\n",
        "  def __init__(self, n_features, n_classes):\n",
        "    self.model = Sequential()\n",
        "    self.model.add(Embedding(input_dim=n_features,output_dim=128,input_length=n_features))\n",
        "    self.model.add(Conv1D(64, kernel_size=3, padding='same', activation='relu'))\n",
        "    self.model.add(MaxPooling1D(pool_size=3))\n",
        "    self.model.add(Conv1D(64, kernel_size=4, padding='same', activation='relu'))\n",
        "    self.model.add(MaxPooling1D(pool_size=4))\n",
        "    self.model.add(Flatten())\n",
        "    self.model.add(Dense(1000, activation = 'relu'))\n",
        "    self.model.add(Dense(100, activation = 'relu'))\n",
        "    self.model.add(Dense(n_classes, activation='softmax'))\n",
        "    if n_classes == 2:\n",
        "      self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', precision, recall, f1_score])\n",
        "    else:\n",
        "      self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', precision, recall, f1_score])\n",
        "\n",
        "  def fit(self, x_train, y_train, e, bs, v):\n",
        "    return self.model.fit(x_train, y_train, epochs = e, batch_size = bs, verbose = v)\n",
        "\n",
        "  def evaluate(self, x_test, y_test):\n",
        "    return self.model.evaluate(x_test, y_test)\n",
        "    \n",
        "  def save(self, file_path):\n",
        "    self.model.save(file_path)\n",
        "  def load_weights(self, file_path):\n",
        "    self.model.load_weights(file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NR36ZU9ISSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training of single channel CNN model\n",
        "x_train,x_test,y_train,y_test = get_data(bag_of_words = False, tfidf = True, max_features=3000)\n",
        "\n",
        "n_features = x_train.shape[1]\n",
        "n_classes = y_train.shape[1]\n",
        "model = CnnModel(n_features, n_classes)\n",
        "history = model.fit(x_train, y_train, e = 10, bs = 512,v = 1)\n",
        "# model.load_weights('1093805_1dconv_reg_tfidf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJmcreFgnkO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('1093805_1dconv_reg_tfidf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_VMIaP_x0YR",
        "colab_type": "code",
        "outputId": "30d7fb38-0cbe-47cf-e7aa-4579f31e09dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "training_accuracy = history.history['acc']\n",
        "training_precision = history.history['precision']\n",
        "training_recall = history.history['recall']\n",
        "training_f1_score = history.history['f1_score']\n",
        "\n",
        "avg_training_accuracy = sum(training_accuracy)/ len(training_accuracy)\n",
        "avg_training_precision = sum(training_precision) / len(training_precision)\n",
        "avg_training_recall = sum(training_recall) / len(training_recall)\n",
        "avg_training_f1_score = sum(training_f1_score)/ len(training_f1_score)\n",
        "\n",
        "print('Average Training Accuracy : {0:.2f}'.format(avg_training_accuracy * 100))\n",
        "print('Average Training Precision : {0:.2f}'.format(avg_training_precision ))\n",
        "print('Average Training Recall : {0:.2f}'.format(avg_training_recall ))\n",
        "print('Average Training F1 score : {0:.2f}'.format(avg_training_f1_score ))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Training Accuracy : 51.85\n",
            "Average Training Precision : 0.69\n",
            "Average Training Recall : 0.17\n",
            "Average Training F1 score : 0.26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi-MLrtYK04B",
        "colab_type": "code",
        "outputId": "e6fd5264-4c0b-4098-cb71-63962cd378ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Testing\n",
        "evaluation_history = model.evaluate(x_test, y_test)\n",
        "\n",
        "testing_accuracy = evaluation_history[1]\n",
        "testing_precision = evaluation_history[2]\n",
        "testing_recall = evaluation_history[3]\n",
        "testing_f1_score = evaluation_history[4]\n",
        "print('Testing Accuracy : {0:.2f}'.format(testing_accuracy * 100))\n",
        "print('Testing Precision : {0:.2f}'.format(testing_precision ))\n",
        "print('Testing Recall : {0:.2f}'.format(testing_recall ))\n",
        "print('Testing F1 score : {0:.2f}'.format(testing_f1_score ))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46818/46818 [==============================] - 20s 426us/step\n",
            "Testing Accuracy : 51.97\n",
            "Testing Precision : 0.70\n",
            "Testing Recall : 0.15\n",
            "Testing F1 score : 0.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDNoE8kYIFB5",
        "colab_type": "text"
      },
      "source": [
        "### Multi channel Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d29nGQKNII9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Conv1D,Dropout, MaxPooling1D, Flatten, Dense, Embedding, SpatialDropout1D, Input\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "class MultiCnn():\n",
        "  def __init__(self, n_features, vocab_size, n_classes):\n",
        "    # channel 1\n",
        "    inputs1 = Input(shape=(n_features,))\n",
        "    embedding1 = Embedding(vocab_size, 128)(inputs1)\n",
        "    conv1_1 = Conv1D(filters=64, kernel_size=3, activation='relu')(embedding1)\n",
        "    pool1_1 = MaxPooling1D(pool_size=2)(conv1_1)\n",
        "    conv1_2 = Conv1D(filters=32, kernel_size=3, activation='relu')(pool1_1)\n",
        "    pool1_2 = MaxPooling1D(pool_size=2)(conv1_2)\n",
        "    flat1 = Flatten()(pool1_2)\n",
        "  \n",
        "    # channel 2\n",
        "    inputs2 = Input(shape=(n_features,))\n",
        "    embedding2 = Embedding(vocab_size, 128)(inputs2)\n",
        "    conv2_1 = Conv1D(filters=64, kernel_size=3, activation='relu')(embedding2)\n",
        "    pool2_1 = MaxPooling1D(pool_size=2)(conv2_1)\n",
        "    conv2_2 = Conv1D(filters=32, kernel_size=3, activation='relu')(pool2_1)\n",
        "    pool2_2 = MaxPooling1D(pool_size=2)(conv2_2)\n",
        "    flat2 = Flatten()(pool2_2)\n",
        "\n",
        "    # merge\n",
        "    merged = concatenate([flat1, flat2])\n",
        "\n",
        "    # interpretation\n",
        "    dense1 = Dense(1000, activation='relu')(merged)\n",
        "    outputs = Dense(n_classes, activation='sigmoid')(dense1)\n",
        "    self.model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\n",
        "    # compile model\n",
        "    if n_classes == 2:\n",
        "      self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', precision, recall, f1_score])\n",
        "    else:\n",
        "      self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', precision, recall, f1_score])\n",
        "    \n",
        "  def fit(self, x_train_1,x_train_2, y_train, e, bs, v):\n",
        "    return self.model.fit([x_train_1,x_train_2], y_train, epochs=e, batch_size=bs, verbose = v)\n",
        "\n",
        "  def evaluate(self, x_test_1, x_test_2, y_test):\n",
        "    return self.model.evaluate([x_test_1,x_test_2], y_test)\n",
        "  def save(self, file_path):\n",
        "    self.model.save(file_path)\n",
        "  def load_weights(self, file_path):\n",
        "    self.model.load_weights(file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6NMcY5ngUcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "x_train, x_test, y_train, y_test = train_test_split(documents[:, 0], documents[:,1], random_state = 2003, test_size = 0.3)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "x_train = preprocess_text(x_train)\n",
        "x_test = preprocess_text(x_test)\n",
        "\n",
        "max_features = 3000\n",
        "bag_of_words_x_train, bag_of_words_x_test = get_bag_of_words(x_train, x_test, max_features=max_features)\n",
        "tfidf_x_train, tfidf_x_test = get_tfidf(x_train, x_test, max_features=max_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCweIe5ukZqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = max_features\n",
        "n_classes = y_train.shape[1]\n",
        "multi_cnn_model = MultiCnn(max_features, max_features, n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "aa59babc-421d-455b-80ae-237822f6023b",
        "id": "J2UDApRgmcNx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "# Training of Multi Channel CNN model\n",
        "fit_history = multi_cnn_model.fit(bag_of_words_x_train, tfidf_x_train, y_train, e = 10, bs = 512, v = 1)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "109242/109242 [==============================] - 208s 2ms/step - loss: 1.1508 - acc: 0.5353 - precision: 0.4714 - recall: 0.6757 - f1_score: 0.5482\n",
            "Epoch 2/10\n",
            "109242/109242 [==============================] - 203s 2ms/step - loss: 0.9798 - acc: 0.6103 - precision: 0.5715 - recall: 0.6522 - f1_score: 0.6083\n",
            "Epoch 3/10\n",
            "109242/109242 [==============================] - 199s 2ms/step - loss: 0.9055 - acc: 0.6400 - precision: 0.6172 - recall: 0.6214 - f1_score: 0.6186\n",
            "Epoch 4/10\n",
            "109242/109242 [==============================] - 199s 2ms/step - loss: 0.8376 - acc: 0.6636 - precision: 0.6345 - recall: 0.6382 - f1_score: 0.6353\n",
            "Epoch 5/10\n",
            "109242/109242 [==============================] - 199s 2ms/step - loss: 0.7773 - acc: 0.6839 - precision: 0.6438 - recall: 0.6797 - f1_score: 0.6604\n",
            "Epoch 6/10\n",
            "109242/109242 [==============================] - 199s 2ms/step - loss: 0.7313 - acc: 0.6972 - precision: 0.6456 - recall: 0.7172 - f1_score: 0.6789\n",
            "Epoch 7/10\n",
            "109242/109242 [==============================] - 199s 2ms/step - loss: 0.6936 - acc: 0.7077 - precision: 0.6439 - recall: 0.7586 - f1_score: 0.6959\n",
            "Epoch 8/10\n",
            "109242/109242 [==============================] - 199s 2ms/step - loss: 0.6668 - acc: 0.7155 - precision: 0.6417 - recall: 0.7866 - f1_score: 0.7064\n",
            "Epoch 9/10\n",
            "109242/109242 [==============================] - 199s 2ms/step - loss: 0.6464 - acc: 0.7208 - precision: 0.6413 - recall: 0.8076 - f1_score: 0.7146\n",
            "Epoch 10/10\n",
            "109242/109242 [==============================] - 199s 2ms/step - loss: 0.6292 - acc: 0.7250 - precision: 0.6384 - recall: 0.8259 - f1_score: 0.7199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Pkjh900d-C",
        "colab_type": "code",
        "outputId": "1856f0cf-f398-4ddd-86b9-6110cd0f3e3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "training_accuracy = fit_history.history['acc']\n",
        "training_precision = fit_history.history['precision']\n",
        "training_recall = fit_history.history['recall']\n",
        "training_f1_score = fit_history.history['f1_score']\n",
        "\n",
        "avg_training_accuracy = sum(training_accuracy)/ len(training_accuracy)\n",
        "avg_training_precision = sum(training_precision) / len(training_precision)\n",
        "avg_training_recall = sum(training_recall) / len(training_recall)\n",
        "avg_training_f1_score = sum(training_f1_score)/ len(training_f1_score)\n",
        "\n",
        "print('Average Training Accuracy : {0:.2f}'.format(avg_training_accuracy * 100))\n",
        "print('Average Training Precision : {0:.2f}'.format(avg_training_precision))\n",
        "print('Average Training Recall : {0:.2f}'.format(avg_training_recall))\n",
        "print('Average Training F1 score : {0:.2f}'.format(avg_training_f1_score))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Training Accuracy : 66.99\n",
            "Average Training Precision : 0.61\n",
            "Average Training Recall : 0.72\n",
            "Average Training F1 score : 0.66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJoQUy7pzFmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('1093805_1dconv_reg_multi_channel')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9It4b9RmrffP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_history = multi_cnn_model.evaluate(bag_of_words_x_test, tfidf_x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm9OZQQ00ffD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_accuracy = testing_history[1]\n",
        "testing_precision = testing_history[2]\n",
        "testing_recall = testing_history[3]\n",
        "testing_f1_score = testing_history[4]\n",
        "print('Testing Accuracy : {0:.2f}'.format(testing_accuracy * 100))\n",
        "print('Testing Precision : {0:.2f}'.format(testing_precision ))\n",
        "print('Testing Recall : {0:.2f}'.format(testing_recall))\n",
        "print('Testing F1 score : {0:.2f}'.format(testing_f1_score))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}